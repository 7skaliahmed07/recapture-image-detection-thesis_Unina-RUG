# ğŸŒ Image Recapture Detection

## ğŸ’¡ Introduction

This project studies how to tell if an image is **original** or a **recaptured photo**.
A recaptured image is a photo taken of a **digital screen** such as a **laptop, tablet, or smartphone**.
The goal is to detect small visual clues that show whether an image was taken from a screen or is an original digital file.

---

## ğŸ” What is Recapture Detection

Recapture detection helps computers find out if a photo was taken from another screen.
When a person takes a picture of a monitor, tiny marks appear because of **screen pixels**, **light reflections**, or **camera focus**.
These small changes help identify a recaptured image.

---

## ğŸ”’ Why It Is Important

1ï¸âƒ£ Helps prevent the stealing of important digital data.
2ï¸âƒ£ Useful in finding **fake screenshots** or **fake ID photos**.
3ï¸âƒ£ Supports **digital security** and **forensic investigations**.

---

## ğŸ“š Papers Studied

Before building the models, six main research papers were reviewed.
Each used different ideas like **chromaticity maps**, **adversarial learning**, and **attention networks**.
These papers helped decide the right direction for this project.

### Main Ideas from the Papers

âœ¨ Studying how colors change when screens are photographed.
âœ¨ Using neural networks that can tell clean and recaptured images apart.
âœ¨ Observing light reflections and pixel patterns.
âœ¨ Mixing local and global details for better results.

---

## ğŸ§  Technical Words Explained

ğŸ **MoirÃ© Pattern** â€“ Wavy lines that appear when a screen is photographed.
ğŸ¨ **Color Artifacts** â€“ Unnatural colors near edges in screen photos.
ğŸ”Š **Fourier Transform** â€“ Looks at frequency patterns in an image.
âœ‚ï¸ **Laplacian Filter** â€“ Highlights edges and small details.
ğŸ” **Transfer Learning** â€“ Using a pre-trained model to improve accuracy and save time.

---

## âš™ï¸ Method Used

### ğŸ”¸ Step 1: Image Feature Extraction

Two image processing methods were used:

ğŸ“ˆ **Fourier Transform**

* Changes the image into frequency form.
* Finds repeating patterns like moirÃ© lines.

ğŸ§© **Laplacian Filter**

* Detects edges and fine details.
* Helps find blur and unnatural sharpness from screen photos.

---

### ğŸ”¸ Step 2: Custom CNN Model

A simple **Convolutional Neural Network (CNN)** was built to classify images as *original* or *recaptured*.
It includes:

* Four convolution layers
* Global average pooling
* Batch normalization
* Dropout for regularization
* Dense output layer with sigmoid activation

This model is lightweight and easy to train on medium datasets.

---

### ğŸ”¸ Step 3: Transfer Learning Models

Two advanced models were also used to improve accuracy.

ğŸ“± **MobileNetV2**

* Fast and small model.
* Works well with limited data.
* Suitable for real-world mobile applications.

âš¡ **EfficientNet**

* High accuracy with fewer parameters.
* Keeps fine image textures.
* Great for detailed recapture detection tasks.

---

## ğŸ§ª Experiments Conducted

Eight experiments were done using different model and image combinations:

1ï¸âƒ£ Basic image analysis and preprocessing
2ï¸âƒ£ CNN on Fourier images
3ï¸âƒ£ MobileNetV2 on Fourier images
4ï¸âƒ£ MobileNetV2 on Laplacian images
5ï¸âƒ£ MobileNetV2 on both Fourier and Laplacian images
6ï¸âƒ£ MobileNetV2 on RGB (normal) images
7ï¸âƒ£ EfficientNet on RGB images
8ï¸âƒ£ EfficientNet on both Laplacian and RGB images

---

## ğŸ“Š Results

ğŸ¥‡ **RGB + Laplacian EfficientNetB0** â€” 90% Accuracy
â­ Best overall model with balanced precision and recall
â­ Combines both RGB and Laplacian image inputs
â­ High accuracy but uses more parameters (8.4M)

ğŸ¥ˆ **RGB EfficientNetB0** â€” 87.7% Accuracy
â­ Strong single-stream model using only RGB images
â­ Balanced performance and lower complexity

ğŸ¥‰ **RGB MobileNetV2** â€” 85.7% Accuracy
â­ Very lightweight model
â­ Ideal for mobile or real-time use

4ï¸âƒ£ **Laplacian MobileNetV2** â€” 84.6% Accuracy
â­ Uses only edge information
â­ Most efficient model for low-resource systems

---

## ğŸ§° How to Use

All code is written in **Google Colab**.

1ï¸âƒ£ Open any notebook from the `src` folder.
2ï¸âƒ£ Install the required libraries listed in `requirements.txt`.
3ï¸âƒ£ Run all the cells to train and test the models.

---

## ğŸ“ Credits

**Faculty of Science and Engineering**
**Information Systems â€” Bernoulli Institute**

**Masterâ€™s Thesis by:**
ğŸ‘¨â€ğŸ“ **Uzer Ahmed**

**Affiliations:**
ğŸ› **PRISMA Research Lab, University of Groningen, The Netherlands**
ğŸ› **Department of Electrical Engineering and Information Technology, University of Naples Federico II, Italy**

**Supervisors:**
ğŸ‘¨â€ğŸ« **Prof. George Azzopardi** â€” University of Groningen, The Netherlands
ğŸ‘¨â€ğŸ« **Dr. Guru Swaroop Bennabhatkula** â€” University of Groningen, The Netherlands

---

This project focuses only on **images taken from digital screens** such as **laptops, smartphones, and tablets** â€” not from paper or printed copies.
