
---

## ğŸ§° How to Use

All code is written in **Google Colab**.

1ï¸âƒ£ Open any notebook from the `src` folder.
2ï¸âƒ£ Install the required libraries listed in `requirements.txt`.
3ï¸âƒ£ Run all the cells to train and test the models.

---

## ğŸ“ Credits

**Faculty of Science and Engineering**

**Information Systems â€” Bernoulli Institute**

**Masterâ€™s Thesis by:**
ğŸ‘¨â€ğŸ“ **Uzer Ahmed**

**Affiliations:**
ğŸ› **PRISMA Research Lab, University of Groningen, The Netherlands**

ğŸ› **Department of Electrical EngineeringFmt and Information Technology, University of Naples Federico II, Italy**

**Supervisors:**
ğŸ‘¨â€ğŸ« **Prof. George Azzopardi** â€” University of Groningen, The Netherlands

ğŸ‘¨â€ğŸ« **Dr. Guru Swaroop Bennabhatkula** â€” University of Groningen, The Netherlands

---

This project focuses only on **images taken from digital screens** such as **laptops, smartphones, and tablets** â€” not from paper or printed copies.

-------------------------------------------------------------------------------------------

1. **Image-Analysis (Stage-1)**  
   Performed initial exploratory data analysis on NTU-Roselab dataset.  
   Visualized originals vs recaptures using histograms, edges, and frequency domain.

2. **Fourier CNN Model**  
   Applied 2D FFT to images and trained a custom CNN on magnitude spectrum.  
   Aimed to detect moirÃ© patterns typical in screen recaptures.

3. **Fourier-MobileNetV2**  
   Preprocessed images with FFT and fed log-magnitude to MobileNetV2.  
   Lightweight model for frequency-based recapture detection.

4. **Laplacian-MobileNetV2**  
   Enhanced edge information using Laplacian filter before MobileNetV2.  
   Focused on sharpness differences between original and recaptured images.

5. **FFT & Laplacian-MobileNetV2**  
   Combined FFT (frequency) and Laplacian (edges) as 2-channel input.  
   Multi-domain feature fusion for improved recapture classification.

6. **RGB-MobileNetV2**  
   Trained MobileNetV2 directly on raw RGB images (no preprocessing).  
   Baseline spatial-domain model using transfer learning.

7. **RGB-EfficientNetB0**  
   Used EfficientNetB0 on RGB input for higher accuracy and efficiency.  
   Stronger backbone for learning complex visual patterns.

8. **RGB-EfficientNetB0-Laplacian**  
   Concatenated RGB and Laplacian channels (4-channel input).  
   Hybrid model leveraging both color and edge sharpness cues.

9. **RGB-EfficientNetB0-Laplacian (Roselab + Android-captured)**  
   Merged NTU-Roselab with real Android-captured recaptures. Laplacian filter was applied but did not improve performance significantly.

10. **RGB-EfficientNetB0 (Roselab + Android-captured)**  
    Trained on merged dataset using only RGB (removed Laplacian). Model showed bias toward NTU-Roselab images. ~82% val accuracy.

11. **RGB-EfficientNetB0 (NTU-Android x8) + iPhone testing**  
    Multiplied Android-captured images by 8 (1600 total) to balance NTU-Roselab (2400). Trained on RGB only. Excellent generalization; tested successfully on unseen iPhone recaptures. Achieved 92% accuracy.