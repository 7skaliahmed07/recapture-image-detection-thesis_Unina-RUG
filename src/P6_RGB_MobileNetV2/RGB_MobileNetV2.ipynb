{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vst2BVxWxqRL",
        "outputId": "cceab045-0caf-4144-dc7c-f6905da10893"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Dataset Balance: 1202 originals, 1199 recaptures\n",
            "Loading existing train-test split...\n",
            "\n",
            "Training Fold 1/5\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
            "\u001b[1m9406464/9406464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Epoch 1/8\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 92ms/step - accuracy: 0.5709 - loss: 0.8704 - val_accuracy: 0.6771 - val_loss: 0.6718\n",
            "Epoch 2/8\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 56ms/step - accuracy: 0.7658 - loss: 0.5653 - val_accuracy: 0.6823 - val_loss: 0.8121\n",
            "Epoch 3/8\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 37ms/step - accuracy: 0.8637 - loss: 0.4165 - val_accuracy: 0.7135 - val_loss: 0.8620\n",
            "Epoch 4/8\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 39ms/step - accuracy: 0.8940 - loss: 0.3503 - val_accuracy: 0.7083 - val_loss: 0.8255\n",
            "Predictions _fold_1: 96 originals, 288 recaptures\n",
            "\n",
            "Training Fold 2/5\n",
            "Epoch 1/8\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 104ms/step - accuracy: 0.5833 - loss: 0.8660 - val_accuracy: 0.7057 - val_loss: 0.6174\n",
            "Epoch 2/8\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 40ms/step - accuracy: 0.7863 - loss: 0.5582 - val_accuracy: 0.7161 - val_loss: 0.6951\n",
            "Epoch 3/8\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 55ms/step - accuracy: 0.8673 - loss: 0.3949 - val_accuracy: 0.6953 - val_loss: 0.7682\n",
            "Epoch 4/8\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.8836 - loss: 0.3426 - val_accuracy: 0.6875 - val_loss: 1.0285\n",
            "Predictions _fold_2: 127 originals, 257 recaptures\n",
            "\n",
            "Training Fold 3/5\n",
            "Epoch 1/8\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 102ms/step - accuracy: 0.6103 - loss: 0.8368 - val_accuracy: 0.7604 - val_loss: 0.5085\n",
            "Epoch 2/8\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 46ms/step - accuracy: 0.7671 - loss: 0.5659 - val_accuracy: 0.7760 - val_loss: 0.4811\n",
            "Epoch 3/8\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 38ms/step - accuracy: 0.8339 - loss: 0.4347 - val_accuracy: 0.7891 - val_loss: 0.5281\n",
            "Epoch 4/8\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 56ms/step - accuracy: 0.8583 - loss: 0.3704 - val_accuracy: 0.8021 - val_loss: 0.4791\n",
            "Epoch 5/8\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 39ms/step - accuracy: 0.9090 - loss: 0.2914 - val_accuracy: 0.7995 - val_loss: 0.5159\n",
            "Epoch 6/8\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 47ms/step - accuracy: 0.9234 - loss: 0.2363 - val_accuracy: 0.8568 - val_loss: 0.3836\n",
            "Epoch 7/8\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 55ms/step - accuracy: 0.9658 - loss: 0.1426 - val_accuracy: 0.8490 - val_loss: 0.3902\n",
            "Epoch 8/8\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 45ms/step - accuracy: 0.9715 - loss: 0.1231 - val_accuracy: 0.8594 - val_loss: 0.3630\n",
            "Predictions _fold_3: 196 originals, 188 recaptures\n",
            "\n",
            "Training Fold 4/5\n",
            "Epoch 1/8\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 101ms/step - accuracy: 0.5895 - loss: 0.8537 - val_accuracy: 0.7240 - val_loss: 0.5657\n",
            "Epoch 2/8\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 39ms/step - accuracy: 0.7545 - loss: 0.6000 - val_accuracy: 0.6745 - val_loss: 0.7519\n",
            "Epoch 3/8\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 48ms/step - accuracy: 0.8336 - loss: 0.4557 - val_accuracy: 0.6771 - val_loss: 0.8468\n",
            "Epoch 4/8\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 42ms/step - accuracy: 0.8886 - loss: 0.3538 - val_accuracy: 0.7161 - val_loss: 0.8599\n",
            "Predictions _fold_4: 124 originals, 260 recaptures\n",
            "\n",
            "Training Fold 5/5\n",
            "Epoch 1/8\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 98ms/step - accuracy: 0.6213 - loss: 0.8229 - val_accuracy: 0.6641 - val_loss: 0.6928\n",
            "Epoch 2/8\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.7764 - loss: 0.5509 - val_accuracy: 0.6536 - val_loss: 0.9857\n",
            "Epoch 3/8\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 39ms/step - accuracy: 0.8369 - loss: 0.4194 - val_accuracy: 0.5807 - val_loss: 1.6391\n",
            "Epoch 4/8\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 54ms/step - accuracy: 0.8938 - loss: 0.3311 - val_accuracy: 0.6510 - val_loss: 1.3744\n",
            "Predictions _fold_5: 90 originals, 294 recaptures\n",
            "\n",
            "Training final model on full training set\n",
            "Epoch 1/8\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 43ms/step - accuracy: 0.6321 - loss: 0.7931\n",
            "Epoch 2/8\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 50ms/step - accuracy: 0.8203 - loss: 0.4983\n",
            "Epoch 3/8\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 38ms/step - accuracy: 0.8631 - loss: 0.3992\n",
            "Epoch 4/8\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 41ms/step - accuracy: 0.9050 - loss: 0.2968\n",
            "Epoch 5/8\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 46ms/step - accuracy: 0.9353 - loss: 0.2181\n",
            "Epoch 6/8\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 38ms/step - accuracy: 0.9440 - loss: 0.1938\n",
            "Epoch 7/8\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 49ms/step - accuracy: 0.9601 - loss: 0.1435\n",
            "Epoch 8/8\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 37ms/step - accuracy: 0.9610 - loss: 0.1271\n",
            "Predictions : 274 originals, 207 recaptures\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ],
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final Results for RGB_MobileNetV2: {'Model': 'RGB_MobileNetV2', 'Preprocessing': 'RGB', 'Accuracy': 0.8565488565488566, 'Total_Parameters': 2422081, 'Trainable_Parameters': 1370177, 'Fold': 'Final', 'learning_rate': 0.0001, 'batch_size': 16, 'optimizer': 'Adam', 'epochs': 8, 'n_folds': 5, 'dropout_rate': 0.4, 'Precision_originals': 0.8138686131386861, 'Recall_originals': 0.9253112033195021, 'F1-Score_originals': 0.8660194174757282, 'Support_originals': 241.0, 'Precision_recaptured': 0.9130434782608695, 'Recall_recaptured': 0.7875, 'F1-Score_recaptured': 0.8456375838926175, 'Support_recaptured': 240.0, 'Precision_macro avg': 0.8634560456997777, 'Recall_macro avg': 0.856405601659751, 'F1-Score_macro avg': 0.8558285006841728, 'Support_macro avg': 481.0, 'Precision_weighted avg': 0.8633529533243909, 'Recall_weighted avg': 0.8565488565488566, 'F1-Score_weighted avg': 0.8558496876213695, 'Support_weighted avg': 481.0}\n",
            "\n",
            "Cross-Validation Summary: {'Model': 'RGB_MobileNetV2', 'Preprocessing': 'RGB', 'Mean_Accuracy': 0.7260416666666667, 'Std_Accuracy': 0.07815537603914043, 'Mean_Precision_recaptured': 0.6819200244922278, 'Mean_Recall_recaptured': 0.8968041012216406, 'Mean_F1-Score_recaptured': 0.7694796738220072, 'Mean_Total_Parameters': 2422081.0, 'Mean_Trainable_Parameters': 1370177.0}\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D, Input\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from pathlib import Path\n",
        "import json\n",
        "\n",
        "# Step 1: Define paths and hyperparameters\n",
        "DATA_ROOT = \"/content/drive/MyDrive/NTU-Roselab-Dataset\"\n",
        "IMG_SIZE = 224\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 8\n",
        "LEARNING_RATE = 1e-4\n",
        "N_FOLDS = 5\n",
        "MODEL_NAME = \"RGB_EfficientNetB0\"\n",
        "OUTPUT_DIR = f\"/content/drive/MyDrive/Recapture_Photo_Detection/{MODEL_NAME}/results\"\n",
        "SPLIT_DIR = \"/content/drive/MyDrive/Recapture_Photo_Detection\"\n",
        "CHECKPOINT_FILE = f\"{OUTPUT_DIR}/checkpoint.json\"\n",
        "PREPROCESSING = \"RGB\"\n",
        "HYPERPARAMETERS = {\n",
        "    \"learning_rate\": LEARNING_RATE,\n",
        "    \"batch_size\": BATCH_SIZE,\n",
        "    \"optimizer\": \"Adam\",\n",
        "    \"epochs\": EPOCHS,\n",
        "    \"n_folds\": N_FOLDS,\n",
        "    \"dropout_rate\": 0.4\n",
        "}\n",
        "\n",
        "# Step 2: Mount Google Drive and verify dataset path\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "except ImportError:\n",
        "    raise ImportError(\"This script must be run in Google Colab with Google Drive mounted.\")\n",
        "if not os.path.exists(DATA_ROOT):\n",
        "    raise FileNotFoundError(f\"Dataset directory {DATA_ROOT} does not exist. Please check the path.\")\n",
        "\n",
        "# Step 3: Check dataset balance\n",
        "def check_dataset_balance(data_root):\n",
        "    originals_path = os.path.join(data_root, 'originals')\n",
        "    recaptures_path = os.path.join(data_root, 'recaptures')\n",
        "    originals_count = sum(len(files) for _, _, files in os.walk(originals_path))\n",
        "    recaptures_count = sum(len(files) for _, _, files in os.walk(recaptures_path))\n",
        "    print(f\"Dataset Balance: {originals_count} originals, {recaptures_count} recaptures\")\n",
        "    return originals_count, recaptures_count\n",
        "\n",
        "originals_count, recaptures_count = check_dataset_balance(DATA_ROOT)\n",
        "\n",
        "# Step 4: Define preprocessing function\n",
        "@tf.function\n",
        "def preprocess(img, label):\n",
        "    img = tf.image.resize(img, (IMG_SIZE, IMG_SIZE))\n",
        "    img = tf.keras.applications.efficientnet.preprocess_input(img)\n",
        "    return img, label\n",
        "\n",
        "# Step 5: Define custom rotation function\n",
        "@tf.function\n",
        "def random_rotation(img, max_angle=0.1):  # max_angle in radians (~10 degrees)\n",
        "    angles = [0, np.pi/2, np.pi, 3*np.pi/2]  # 0°, 90°, 180°, 270°\n",
        "    k = tf.random.uniform(shape=(), minval=0, maxval=len(angles), dtype=tf.int32)\n",
        "    img = tf.image.rot90(img, k)\n",
        "    return img\n",
        "\n",
        "# Step 6: Define data augmentation\n",
        "@tf.function\n",
        "def augment_image(img, label):\n",
        "    img = tf.image.random_flip_left_right(img)\n",
        "    img = random_rotation(img, max_angle=0.1)\n",
        "    return img, label\n",
        "\n",
        "# Step 7: Load or create train-test split\n",
        "def load_or_create_split():\n",
        "    split_files = ['X_train.npy', 'X_test.npy', 'y_train.npy', 'y_test.npy']\n",
        "    if all(os.path.exists(os.path.join(SPLIT_DIR, f)) for f in split_files):\n",
        "        print(\"Loading existing train-test split...\")\n",
        "        X_train = np.load(os.path.join(SPLIT_DIR, 'X_train.npy'))\n",
        "        X_test = np.load(os.path.join(SPLIT_DIR, 'X_test.npy'))\n",
        "        y_train = np.load(os.path.join(SPLIT_DIR, 'y_train.npy'))\n",
        "        y_test = np.load(os.path.join(SPLIT_DIR, 'y_test.npy'))\n",
        "    else:\n",
        "        print(\"Creating new train-test split...\")\n",
        "        dataset = image_dataset_from_directory(\n",
        "            DATA_ROOT,\n",
        "            labels='inferred',\n",
        "            label_mode='binary',\n",
        "            image_size=(IMG_SIZE, IMG_SIZE),\n",
        "            batch_size=BATCH_SIZE,\n",
        "            shuffle=True,\n",
        "            seed=42\n",
        "        )\n",
        "        images, labels = [], []\n",
        "        for img_batch, label_batch in dataset:\n",
        "            images.append(img_batch.numpy())\n",
        "            labels.append(label_batch.numpy())\n",
        "        images = np.concatenate(images, axis=0)\n",
        "        labels = np.concatenate(labels, axis=0).flatten()\n",
        "\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            images, labels, test_size=0.2, stratify=labels, random_state=42\n",
        "        )\n",
        "\n",
        "        Path(SPLIT_DIR).mkdir(parents=True, exist_ok=True)\n",
        "        np.save(os.path.join(SPLIT_DIR, 'X_train.npy'), X_train)\n",
        "        np.save(os.path.join(SPLIT_DIR, 'X_test.npy'), X_test)\n",
        "        np.save(os.path.join(SPLIT_DIR, 'y_train.npy'), y_train)\n",
        "        np.save(os.path.join(SPLIT_DIR, 'y_test.npy'), y_test)\n",
        "\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "X_train, X_test, y_train, y_test = load_or_create_split()\n",
        "\n",
        "# Create test dataset\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(BATCH_SIZE).map(preprocess).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# Step 8: Define function to create RGB EfficientNetB0 model\n",
        "def create_model():\n",
        "    input_layer = Input(shape=(IMG_SIZE, IMG_SIZE, 3), name='rgb_input')\n",
        "    base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(IMG_SIZE, IMG_SIZE, 3))\n",
        "    for layer in base_model.layers[:-20]:\n",
        "        layer.trainable = False\n",
        "    x = GlobalAveragePooling2D()(base_model(input_layer))\n",
        "    x = Dropout(HYPERPARAMETERS['dropout_rate'])(x)\n",
        "    x = Dense(128, activation='relu')(x)\n",
        "    out = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    model = Model(inputs=input_layer, outputs=out)\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(LEARNING_RATE),\n",
        "                  loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Step 9: Convert NumPy types to JSON-serializable types\n",
        "def convert_to_serializable(obj):\n",
        "    if isinstance(obj, np.integer):\n",
        "        return int(obj)\n",
        "    elif isinstance(obj, np.floating):\n",
        "        return float(obj)\n",
        "    elif isinstance(obj, np.ndarray):\n",
        "        return obj.tolist()\n",
        "    return obj\n",
        "\n",
        "# Step 10: Define function to save results\n",
        "def save_model_results(model, dataset, history, model_name, output_dir, fold=None, preprocessing='None', hyperparameters=None):\n",
        "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
        "    fold_str = f\"_fold_{fold}\" if fold is not None else \"\"\n",
        "\n",
        "    # Evaluate on dataset\n",
        "    y_true, y_pred = [], []\n",
        "    for imgs, labels in dataset:\n",
        "        preds = (model.predict(imgs, verbose=0) > 0.5).astype(int)\n",
        "        y_true.extend(labels.numpy().astype(int))\n",
        "        y_pred.extend(preds.flatten())\n",
        "\n",
        "    # Check prediction distribution\n",
        "    originals_pred = sum(1 for p in y_pred if p == 0)\n",
        "    recaptures_pred = sum(1 for p in y_pred if p == 1)\n",
        "    print(f\"Predictions {fold_str}: {originals_pred} originals, {recaptures_pred} recaptures\")\n",
        "\n",
        "    # Classification report\n",
        "    class_report = classification_report(y_true, y_pred, target_names=['originals', 'recaptured'], output_dict=True)\n",
        "    class_report_df = pd.DataFrame(class_report).transpose()\n",
        "    class_report_df.to_csv(f'{output_dir}/{model_name}_classification_report{fold_str}.csv')\n",
        "\n",
        "    # Confusion matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['originals', 'recaptured'], yticklabels=['originals', 'recaptured'])\n",
        "    plt.title(f'Confusion Matrix - {model_name}{fold_str}')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.savefig(f'{output_dir}/{model_name}_confusion_matrix{fold_str}.png')\n",
        "    plt.close()\n",
        "\n",
        "    # Save confusion matrix as CSV\n",
        "    cm_df = pd.DataFrame(cm, index=['True_originals', 'True_recaptured'], columns=['Pred_originals', 'Pred_recaptured'])\n",
        "    cm_df.to_csv(f'{output_dir}/{model_name}_confusion_matrix{fold_str}.csv')\n",
        "\n",
        "    # Model summary (only for final model)\n",
        "    if fold is None:\n",
        "        summary_file = f'{output_dir}/{model_name}_summary.txt'\n",
        "        with open(summary_file, 'w') as f:\n",
        "            model.summary(print_fn=lambda x: f.write(x + '\\n'))\n",
        "\n",
        "    # Calculate total and trainable parameters\n",
        "    total_params = model.count_params()\n",
        "    trainable_params = sum([tf.keras.backend.count_params(w) for w in model.trainable_weights])\n",
        "\n",
        "    # Aggregate results\n",
        "    results = {\n",
        "        'Model': model_name,\n",
        "        'Preprocessing': preprocessing,\n",
        "        'Accuracy': class_report['accuracy'],\n",
        "        'Total_Parameters': total_params,\n",
        "        'Trainable_Parameters': trainable_params,\n",
        "        'Fold': fold if fold is not None else 'Final'\n",
        "    }\n",
        "    if hyperparameters:\n",
        "        results.update(hyperparameters)\n",
        "    for label, metrics in class_report.items():\n",
        "        if isinstance(metrics, dict):\n",
        "            results.update({\n",
        "                f'Precision_{label}': metrics['precision'],\n",
        "                f'Recall_{label}': metrics['recall'],\n",
        "                f'F1-Score_{label}': metrics['f1-score'],\n",
        "                f'Support_{label}': metrics['support']\n",
        "            })\n",
        "\n",
        "    # Convert NumPy types to JSON-serializable types\n",
        "    results = {k: convert_to_serializable(v) for k, v in results.items()}\n",
        "\n",
        "    # Save results to JSON\n",
        "    with open(f'{output_dir}/{model_name}_results{fold_str}.json', 'w') as f:\n",
        "        json.dump(results, f, indent=4)\n",
        "\n",
        "    # Plot and save accuracy/loss curves\n",
        "    if history is not None:\n",
        "        plt.figure(figsize=(10, 4))\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "        if 'val_accuracy' in history.history:\n",
        "            plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "        plt.title(f'Accuracy Curve - {model_name}{fold_str}')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Accuracy')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.plot(history.history['loss'], label='Train Loss')\n",
        "        if 'val_loss' in history.history:\n",
        "            plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "        plt.title(f'Loss Curve - {model_name}{fold_str}')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f'{output_dir}/{model_name}_accuracy_loss_curve{fold_str}.png')\n",
        "        plt.close()\n",
        "\n",
        "    # Save full model for final model (using .keras format)\n",
        "    if fold is None:\n",
        "        model.save(f'{output_dir}/{model_name}_model.keras', overwrite=True)\n",
        "\n",
        "    return results\n",
        "\n",
        "# Step 11: Checkpointing and resumption logic\n",
        "def load_checkpoint():\n",
        "    if os.path.exists(CHECKPOINT_FILE):\n",
        "        with open(CHECKPOINT_FILE, 'r') as f:\n",
        "            checkpoint = json.load(f)\n",
        "        last_completed_fold = checkpoint.get('last_completed_fold', 0)\n",
        "        print(f\"Resuming from checkpoint: Last completed fold = {last_completed_fold}\")\n",
        "        return last_completed_fold\n",
        "    return 0\n",
        "\n",
        "def save_checkpoint(fold):\n",
        "    Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
        "    checkpoint = {'last_completed_fold': fold}\n",
        "    with open(CHECKPOINT_FILE, 'w') as f:\n",
        "        json.dump(checkpoint, f, indent=4)\n",
        "\n",
        "# Step 12: Perform 5-fold cross-validation with checkpointing\n",
        "skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=42)\n",
        "fold_results = []\n",
        "last_completed_fold = load_checkpoint()\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(skf.split(X_train, y_train), 1):\n",
        "    if fold <= last_completed_fold:\n",
        "        print(f\"Skipping fold {fold} (already completed)\")\n",
        "        # Load results from previous run\n",
        "        fold_str = f\"_fold_{fold}\"\n",
        "        result_file = f'{OUTPUT_DIR}/{MODEL_NAME}_results{fold_str}.json'\n",
        "        if os.path.exists(result_file):\n",
        "            with open(result_file, 'r') as f:\n",
        "                fold_results.append(json.load(f))\n",
        "        continue\n",
        "\n",
        "    print(f\"\\nTraining Fold {fold}/{N_FOLDS}\")\n",
        "\n",
        "    # Create train and validation datasets for this fold\n",
        "    X_fold_train, X_fold_val = X_train[train_idx], X_train[val_idx]\n",
        "    y_fold_train, y_fold_val = y_train[train_idx], y_train[val_idx]\n",
        "\n",
        "    train_ds = tf.data.Dataset.from_tensor_slices((X_fold_train, y_fold_train)).batch(BATCH_SIZE).map(preprocess).map(augment_image).prefetch(tf.data.AUTOTUNE)\n",
        "    val_ds = tf.data.Dataset.from_tensor_slices((X_fold_val, y_fold_val)).batch(BATCH_SIZE).map(preprocess).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    # Create model and load weights if available\n",
        "    model = create_model()\n",
        "    fold_str = f\"_fold_{fold}\"\n",
        "    checkpoint_path = f'{OUTPUT_DIR}/{MODEL_NAME}_model{fold_str}.weights.h5'\n",
        "    if os.path.exists(checkpoint_path):\n",
        "        print(f\"Loading weights for fold {fold} from {checkpoint_path}\")\n",
        "        model.load_weights(checkpoint_path)\n",
        "\n",
        "    # Train model with early stopping and checkpointing\n",
        "    checkpoint_callback = ModelCheckpoint(checkpoint_path, monitor='val_loss', save_best_only=True, save_weights_only=True)\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "    history = model.fit(train_ds, validation_data=val_ds, epochs=EPOCHS, class_weight={0: 1.0, 1: 1.5}, verbose=1, callbacks=[early_stopping, checkpoint_callback])\n",
        "\n",
        "    # Save results for this fold\n",
        "    results = save_model_results(\n",
        "        model, val_ds, history, MODEL_NAME, OUTPUT_DIR,\n",
        "        fold=fold, preprocessing=PREPROCESSING, hyperparameters=HYPERPARAMETERS\n",
        "    )\n",
        "    fold_results.append(results)\n",
        "\n",
        "    # Update checkpoint\n",
        "    save_checkpoint(fold)\n",
        "\n",
        "# Step 13: Train final model on full training set if not already done\n",
        "if last_completed_fold < N_FOLDS + 1:\n",
        "    print(\"\\nTraining final model on full training set\")\n",
        "    final_model_path = f'{OUTPUT_DIR}/{MODEL_NAME}_model.keras'\n",
        "    if os.path.exists(final_model_path) and last_completed_fold == 'final':\n",
        "        print(f\"Final model already saved at {final_model_path}, skipping training\")\n",
        "        # Load results from previous run\n",
        "        result_file = f'{OUTPUT_DIR}/{MODEL_NAME}_results.json'\n",
        "        if os.path.exists(result_file):\n",
        "            with open(result_file, 'r') as f:\n",
        "                results = json.load(f)\n",
        "            print(f\"Final Results for {MODEL_NAME}:\", results)\n",
        "    else:\n",
        "        train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(BATCH_SIZE).map(preprocess).map(augment_image).prefetch(tf.data.AUTOTUNE)\n",
        "        model = create_model()\n",
        "        checkpoint_path = f'{OUTPUT_DIR}/{MODEL_NAME}_model.weights.h5'\n",
        "        if os.path.exists(checkpoint_path):\n",
        "            print(f\"Loading weights for final model from {checkpoint_path}\")\n",
        "            model.load_weights(checkpoint_path)\n",
        "\n",
        "        checkpoint_callback = ModelCheckpoint(checkpoint_path, monitor='loss', save_best_only=True, save_weights_only=True)\n",
        "        history = model.fit(train_ds, epochs=EPOCHS, class_weight={0: 1.0, 1: 1.5}, verbose=1, callbacks=[checkpoint_callback])\n",
        "\n",
        "        # Save results for final model\n",
        "        results = save_model_results(\n",
        "            model, test_ds, history, MODEL_NAME, OUTPUT_DIR,\n",
        "            preprocessing=PREPROCESSING, hyperparameters=HYPERPARAMETERS\n",
        "        )\n",
        "        print(f\"Final Results for {MODEL_NAME}:\", results)\n",
        "\n",
        "        # Update checkpoint\n",
        "        save_checkpoint('final')\n",
        "\n",
        "# Step 14: Aggregate cross-validation results\n",
        "if fold_results:\n",
        "    fold_df = pd.DataFrame(fold_results)\n",
        "    mean_results = {\n",
        "        'Model': MODEL_NAME,\n",
        "        'Preprocessing': PREPROCESSING,\n",
        "        'Mean_Accuracy': fold_df['Accuracy'].mean(),\n",
        "        'Std_Accuracy': fold_df['Accuracy'].std(),\n",
        "        'Mean_Precision_recaptured': fold_df['Precision_recaptured'].mean(),\n",
        "        'Mean_Recall_recaptured': fold_df['Recall_recaptured'].mean(),\n",
        "        'Mean_F1-Score_recaptured': fold_df['F1-Score_recaptured'].mean(),\n",
        "        'Mean_Total_Parameters': fold_df['Total_Parameters'].mean(),\n",
        "        'Mean_Trainable_Parameters': fold_df['Trainable_Parameters'].mean()\n",
        "    }\n",
        "    # Convert NumPy types in mean_results\n",
        "    mean_results = {k: convert_to_serializable(v) for k, v in mean_results.items()}\n",
        "    with open(f'{OUTPUT_DIR}/{MODEL_NAME}_cv_summary.json', 'w') as f:\n",
        "        json.dump(mean_results, f, indent=4)\n",
        "    fold_df.to_csv(f'{OUTPUT_DIR}/{MODEL_NAME}_cv_results.csv', index=False)\n",
        "    print(\"\\nCross-Validation Summary:\", mean_results)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
