{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        },
        "id": "HsIjFRdl_iFd",
        "outputId": "6102dab6-2a34-45a6-8f62-6cb46ea183e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Dataset Balance: 1202 originals, 1199 recaptures\n",
            "Loading existing train-test split...\n",
            "Resuming from checkpoint: Last completed fold = 5\n",
            "Skipping fold 1 (already completed)\n",
            "Skipping fold 2 (already completed)\n",
            "Skipping fold 3 (already completed)\n",
            "Skipping fold 4 (already completed)\n",
            "Skipping fold 5 (already completed)\n",
            "\n",
            "Training final model on full training set\n",
            "Loading weights for final model from /content/drive/MyDrive/Recapture_Photo_Detection/RGB_Laplacian_EfficientNetB0/results/RGB_Laplacian_EfficientNetB0_model.weights.h5\n",
            "Epoch 1/8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/saving/saving_lib.py:802: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 854 variables. \n",
            "  saveable.load_own_variables(weights_store.get(inner_path))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m189s\u001b[0m 221ms/step - accuracy: 0.8449 - loss: 0.4623\n",
            "Epoch 2/8\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 234ms/step - accuracy: 0.9091 - loss: 0.2831\n",
            "Epoch 3/8\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 219ms/step - accuracy: 0.9329 - loss: 0.2073\n",
            "Epoch 4/8\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 229ms/step - accuracy: 0.9605 - loss: 0.1228\n",
            "Epoch 5/8\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 243ms/step - accuracy: 0.9709 - loss: 0.0900\n",
            "Epoch 6/8\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 235ms/step - accuracy: 0.9803 - loss: 0.0671\n",
            "Epoch 7/8\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 254ms/step - accuracy: 0.9877 - loss: 0.0447\n",
            "Epoch 8/8\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 213ms/step - accuracy: 0.9890 - loss: 0.0361\n",
            "Predictions : 232 originals, 249 recaptures\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Results for RGB_Laplacian_EfficientNetB0: {'Model': 'RGB_Laplacian_EfficientNetB0', 'Preprocessing': 'RGB_Laplacian', 'Accuracy': 0.9, 'Total_Parameters': 8427079, 'Trainable_Parameters': 8343033, 'Fold': 'Final', 'learning_rate': 0.0001, 'batch_size': 16, 'optimizer': 'Adam', 'epochs': 8, 'n_folds': 5, 'dropout_rate': 0.4, 'Precision_originals': 0.91, 'Recall_originals': 0.88, 'F1-Score_originals': 0.9, 'Support_originals': 241.0, 'Precision_recaptured': 0.88, 'Recall_recaptured': 0.92, 'F1-Score_recaptured': 0.9, 'Support_recaptured': 240.0, 'Precision_macro avg': 0.9, 'Recall_macro avg': 0.9, 'F1-Score_macro avg': 0.9, 'Support_macro avg': 481.0, 'Precision_weighted avg': 0.9, 'Recall_weighted avg': 0.9, 'F1-Score_weighted avg': 0.9, 'Support_weighted avg': 481.0}\n",
            "\n",
            "Cross-Validation Summary: {'Model': 'RGB_Laplacian_EfficientNetB0', 'Preprocessing': 'RGB_Laplacian', 'Mean_Accuracy': np.float64(0.89), 'Std_Accuracy': 0.03, 'Mean_Precision_recaptured': np.float64(0.87), 'Mean_Recall_recaptured': np.float64(0.9), 'Mean_F1-Score_recaptured': np.float64(0.89), 'Mean_Total_Parameters': 8427079, 'Mean_Trainable_Parameters': 8343033}\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Dropout, Conv2D, Concatenate, Input, GlobalAveragePooling2D\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from pathlib import Path\n",
        "import json\n",
        "\n",
        "# Step 1: Define paths and hyperparameters\n",
        "DATA_ROOT = \"/content/drive/MyDrive/NTU-Roselab-Dataset\"\n",
        "IMG_SIZE = 224\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 8\n",
        "LEARNING_RATE = 1e-4\n",
        "N_FOLDS = 5\n",
        "MODEL_NAME = \"RGB_Laplacian_EfficientNetB0\"\n",
        "OUTPUT_DIR = f\"/content/drive/MyDrive/Recapture_Photo_Detection/{MODEL_NAME}/results\"\n",
        "SPLIT_DIR = \"/content/drive/MyDrive/Recapture_Photo_Detection\"\n",
        "CHECKPOINT_FILE = f\"{OUTPUT_DIR}/checkpoint.json\"\n",
        "PREPROCESSING = \"RGB_Laplacian\"\n",
        "HYPERPARAMETERS = {\n",
        "    \"learning_rate\": LEARNING_RATE,\n",
        "    \"batch_size\": BATCH_SIZE,\n",
        "    \"optimizer\": \"Adam\",\n",
        "    \"epochs\": EPOCHS,\n",
        "    \"n_folds\": N_FOLDS,\n",
        "    \"dropout_rate\": 0.4\n",
        "}\n",
        "\n",
        "# Step 2: Mount Google Drive and verify dataset path\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "except ImportError:\n",
        "    raise ImportError(\"This script must be run in Google Colab with Google Drive mounted.\")\n",
        "if not os.path.exists(DATA_ROOT):\n",
        "    raise FileNotFoundError(f\"Dataset directory {DATA_ROOT} does not exist. Please check the path.\")\n",
        "\n",
        "# Step 3: Check dataset balance\n",
        "def check_dataset_balance(data_root):\n",
        "    originals_path = os.path.join(data_root, 'originals')\n",
        "    recaptures_path = os.path.join(data_root, 'recaptures')\n",
        "    originals_count = sum(len(files) for _, _, files in os.walk(originals_path))\n",
        "    recaptures_count = sum(len(files) for _, _, files in os.walk(recaptures_path))\n",
        "    print(f\"Dataset Balance: {originals_count} originals, {recaptures_count} recaptures\")\n",
        "    return originals_count, recaptures_count\n",
        "\n",
        "originals_count, recaptures_count = check_dataset_balance(DATA_ROOT)\n",
        "\n",
        "# Step 4: Define preprocessing functions\n",
        "@tf.function\n",
        "def rgb_preprocess(img):\n",
        "    img = tf.image.resize(img, (IMG_SIZE, IMG_SIZE))\n",
        "    tf.debugging.assert_rank(img, 4)\n",
        "    img = tf.keras.applications.efficientnet.preprocess_input(img)\n",
        "    tf.debugging.assert_rank(img, 4)\n",
        "    return img\n",
        "\n",
        "@tf.function\n",
        "def laplacian_preprocess(img):\n",
        "    img = tf.image.resize(img, (IMG_SIZE, IMG_SIZE))\n",
        "    tf.debugging.assert_rank(img, 4)\n",
        "    img = tf.image.rgb_to_grayscale(img)\n",
        "    tf.debugging.assert_rank(img, 4)\n",
        "    laplacian_kernel = tf.constant([[0, -1, 0],\n",
        "                                    [-1, 4, -1],\n",
        "                                    [0, -1, 0]], dtype=tf.float32)\n",
        "    laplacian_kernel = tf.reshape(laplacian_kernel, [3, 3, 1, 1])\n",
        "    filtered = tf.nn.conv2d(img, laplacian_kernel, strides=[1, 1, 1, 1], padding='SAME')\n",
        "    filtered = tf.abs(filtered)\n",
        "    f_min = tf.reduce_min(filtered)\n",
        "    f_max = tf.reduce_max(filtered)\n",
        "    filtered = (filtered - f_min) / (f_max - f_min + 1e-10)\n",
        "    filtered = tf.tile(filtered, [1, 1, 1, 3])\n",
        "    tf.debugging.assert_rank(filtered, 4)\n",
        "    return filtered\n",
        "\n",
        "@tf.function\n",
        "def preprocess(img, label):\n",
        "    rgb_img = rgb_preprocess(img)\n",
        "    laplacian_img = laplacian_preprocess(img)\n",
        "    tf.debugging.assert_rank(rgb_img, 4)\n",
        "    tf.debugging.assert_rank(laplacian_img, 4)\n",
        "    return (rgb_img, laplacian_img), label\n",
        "\n",
        "# Step 5: Define random rotation\n",
        "@tf.function\n",
        "def random_rotation(img, max_angle=0.1):\n",
        "    angles = [0, np.pi/2, np.pi, 3*np.pi/2]\n",
        "    k = tf.random.uniform(shape=(), minval=0, maxval=len(angles), dtype=tf.int32)\n",
        "    img = tf.image.rot90(img, k)\n",
        "    return img\n",
        "\n",
        "# Step 6: Define data augmentation\n",
        "@tf.function\n",
        "def augment_image(inputs, label):\n",
        "    rgb_img, laplacian_img = inputs\n",
        "    rgb_img = tf.image.random_flip_left_right(rgb_img)\n",
        "    rgb_img = random_rotation(rgb_img)\n",
        "    laplacian_img = tf.image.random_flip_left_right(laplacian_img)\n",
        "    laplacian_img = random_rotation(laplacian_img)\n",
        "    return (rgb_img, laplacian_img), label\n",
        "\n",
        "# Step 7: Load or create train-test split\n",
        "def load_or_create_split():\n",
        "    split_files = ['X_train.npy', 'X_test.npy', 'y_train.npy', 'y_test.npy']\n",
        "    if all(os.path.exists(os.path.join(SPLIT_DIR, f)) for f in split_files):\n",
        "        print(\"Loading existing train-test split...\")\n",
        "        X_train = np.load(os.path.join(SPLIT_DIR, 'X_train.npy'))\n",
        "        X_test = np.load(os.path.join(SPLIT_DIR, 'X_test.npy'))\n",
        "        y_train = np.load(os.path.join(SPLIT_DIR, 'y_train.npy'))\n",
        "        y_test = np.load(os.path.join(SPLIT_DIR, 'y_test.npy'))\n",
        "    else:\n",
        "        print(\"Creating new train-test split...\")\n",
        "        dataset = image_dataset_from_directory(\n",
        "            DATA_ROOT,\n",
        "            labels='inferred',\n",
        "            label_mode='binary',\n",
        "            image_size=(IMG_SIZE, IMG_SIZE),\n",
        "            batch_size=BATCH_SIZE,\n",
        "            shuffle=True,\n",
        "            seed=42\n",
        "        )\n",
        "        images, labels = [], []\n",
        "        for img_batch, label_batch in dataset:\n",
        "            images.append(img_batch.numpy())\n",
        "            labels.append(label_batch.numpy())\n",
        "        images = np.concatenate(images, axis=0)\n",
        "        labels = np.concatenate(labels, axis=0).flatten()\n",
        "\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            images, labels, test_size=0.2, stratify=labels, random_state=42\n",
        "        )\n",
        "\n",
        "        Path(SPLIT_DIR).mkdir(parents=True, exist_ok=True)\n",
        "        np.save(os.path.join(SPLIT_DIR, 'X_train.npy'), X_train)\n",
        "        np.save(os.path.join(SPLIT_DIR, 'X_test.npy'), X_test)\n",
        "        np.save(os.path.join(SPLIT_DIR, 'y_train.npy'), y_train)\n",
        "        np.save(os.path.join(SPLIT_DIR, 'y_test.npy'), y_test)\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "X_train, X_test, y_train, y_test = load_or_create_split()\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(BATCH_SIZE).map(preprocess).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# Step 8: Define the dual EfficientNetB0 model\n",
        "def create_model():\n",
        "    # Inputs\n",
        "    rgb_input = Input(shape=(IMG_SIZE, IMG_SIZE, 3), name='rgb_input')\n",
        "    laplacian_input = Input(shape=(IMG_SIZE, IMG_SIZE, 3), name='laplacian_input')\n",
        "\n",
        "    # RGB branch\n",
        "    rgb_base_model = EfficientNetB0(include_top=False, weights=None, input_shape=(IMG_SIZE, IMG_SIZE, 3), name='efficientnetb0_rgb')\n",
        "    rgb_base_model.load_weights(tf.keras.utils.get_file(\n",
        "        'efficientnetb0_notop.h5',\n",
        "        'https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5'\n",
        "    ))\n",
        "    rgb_base = rgb_base_model(rgb_input)\n",
        "    rgb_base = GlobalAveragePooling2D(name='gap_rgb')(rgb_base)\n",
        "\n",
        "    # Laplacian branch\n",
        "    laplacian_base_model = EfficientNetB0(include_top=False, weights=None, input_shape=(IMG_SIZE, IMG_SIZE, 3), name='efficientnetb0_laplacian')\n",
        "    laplacian_base_model.load_weights(tf.keras.utils.get_file(\n",
        "        'efficientnetb0_notop.h5',\n",
        "        'https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5'\n",
        "    ))\n",
        "    laplacian_base = laplacian_base_model(laplacian_input)\n",
        "    laplacian_base = GlobalAveragePooling2D(name='gap_laplacian')(laplacian_base)\n",
        "\n",
        "    # Combine features\n",
        "    x = Concatenate(name='concat_features')([rgb_base, laplacian_base])\n",
        "    x = Dense(128, activation='relu', name='dense_128')(x)\n",
        "    x = Dropout(HYPERPARAMETERS['dropout_rate'], name='dropout')(x)\n",
        "    out = Dense(1, activation='sigmoid', name='output')(x)\n",
        "\n",
        "    model = Model(inputs=[rgb_input, laplacian_input], outputs=out, name='RGB_Laplacian_EfficientNetB0')\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(LEARNING_RATE),\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "# Step 9: Convert NumPy types to JSON-serializable types and round floats\n",
        "def convert_to_serializable(obj):\n",
        "    if isinstance(obj, np.integer):\n",
        "        return int(obj)\n",
        "    elif isinstance(obj, np.floating):\n",
        "        return round(float(obj), 2)\n",
        "    elif isinstance(obj, np.ndarray):\n",
        "        return obj.tolist()\n",
        "    elif isinstance(obj, np.int64):\n",
        "        return int(obj)\n",
        "    return obj\n",
        "\n",
        "# Step 10: Define function to save results with rounded values\n",
        "def save_model_results(model, dataset, history, model_name, output_dir, fold=None, preprocessing='None', hyperparameters=None):\n",
        "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
        "    fold_str = f\"_fold_{fold}\" if fold is not None else \"\"\n",
        "\n",
        "    y_true, y_pred = [], []\n",
        "    for (rgb_imgs, laplacian_imgs), labels in dataset:\n",
        "        preds = (model.predict([rgb_imgs, laplacian_imgs], verbose=0) > 0.5).astype(int)\n",
        "        y_true.extend(labels.numpy().astype(int))\n",
        "        y_pred.extend(preds.flatten())\n",
        "\n",
        "    originals_pred = sum(1 for p in y_pred if p == 0)\n",
        "    recaptures_pred = sum(1 for p in y_pred if p == 1)\n",
        "    print(f\"Predictions {fold_str}: {originals_pred} originals, {recaptures_pred} recaptures\")\n",
        "\n",
        "    class_report = classification_report(y_true, y_pred, target_names=['originals', 'recaptured'], output_dict=True)\n",
        "    class_report_df = pd.DataFrame(class_report).transpose().round(2)\n",
        "    class_report_df.to_csv(f'{output_dir}/{model_name}_classification_report{fold_str}.csv')\n",
        "\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['originals', 'recaptured'], yticklabels=['originals', 'recaptured'])\n",
        "    plt.title(f'Confusion Matrix - {model_name}{fold_str}')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.savefig(f'{output_dir}/{model_name}_confusion_matrix{fold_str}.png')\n",
        "    plt.close()\n",
        "\n",
        "    cm_df = pd.DataFrame(cm, index=['True_originals', 'True_recaptured'], columns=['Pred_originals', 'Pred_recaptured'])\n",
        "    cm_df.to_csv(f'{output_dir}/{model_name}_confusion_matrix{fold_str}.csv')\n",
        "\n",
        "    if fold is None:\n",
        "        summary_file = f'{output_dir}/{model_name}_summary.txt'\n",
        "        with open(summary_file, 'w') as f:\n",
        "            model.summary(print_fn=lambda x: f.write(x + '\\n'))\n",
        "\n",
        "    total_params = int(model.count_params())\n",
        "    trainable_params = int(sum([tf.keras.backend.count_params(w) for w in model.trainable_weights]))\n",
        "\n",
        "    results = {\n",
        "        'Model': model_name,\n",
        "        'Preprocessing': preprocessing,\n",
        "        'Accuracy': round(class_report['accuracy'], 2),\n",
        "        'Total_Parameters': total_params,\n",
        "        'Trainable_Parameters': trainable_params,\n",
        "        'Fold': fold if fold is not None else 'Final'\n",
        "    }\n",
        "    if hyperparameters:\n",
        "        results.update({k: convert_to_serializable(v) for k, v in hyperparameters.items()})\n",
        "    for label, metrics in class_report.items():\n",
        "        if isinstance(metrics, dict):\n",
        "            results.update({\n",
        "                f'Precision_{label}': round(metrics['precision'], 2),\n",
        "                f'Recall_{label}': round(metrics['recall'], 2),\n",
        "                f'F1-Score_{label}': round(metrics['f1-score'], 2),\n",
        "                f'Support_{label}': convert_to_serializable(metrics['support'])\n",
        "            })\n",
        "\n",
        "    with open(f'{output_dir}/{model_name}_results{fold_str}.json', 'w') as f:\n",
        "        json.dump(results, f, indent=4, default=convert_to_serializable)\n",
        "\n",
        "    if history is not None:\n",
        "        plt.figure(figsize=(10, 4))\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "        if 'val_accuracy' in history.history:\n",
        "            plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "        plt.title(f'Accuracy Curve - {model_name}{fold_str}')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Accuracy')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.plot(history.history['loss'], label='Train Loss')\n",
        "        if 'val_loss' in history.history:\n",
        "            plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "        plt.title(f'Loss Curve - {model_name}{fold_str}')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f'{output_dir}/{model_name}_accuracy_loss_curve{fold_str}.png')\n",
        "        plt.close()\n",
        "\n",
        "    if fold is None:\n",
        "        model.save(f'{output_dir}/{model_name}_model.keras', overwrite=True)\n",
        "\n",
        "    return results\n",
        "\n",
        "# Step 11: Checkpointing and resumption logic\n",
        "def load_checkpoint():\n",
        "    if os.path.exists(CHECKPOINT_FILE):\n",
        "        with open(CHECKPOINT_FILE, 'r') as f:\n",
        "            checkpoint = json.load(f)\n",
        "        last_completed_fold = checkpoint.get('last_completed_fold', 0)\n",
        "        print(f\"Resuming from checkpoint: Last completed fold = {last_completed_fold}\")\n",
        "        return last_completed_fold\n",
        "    return 0\n",
        "\n",
        "def save_checkpoint(fold):\n",
        "    Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
        "    checkpoint = {'last_completed_fold': fold}\n",
        "    with open(CHECKPOINT_FILE, 'w') as f:\n",
        "        json.dump(checkpoint, f, indent=4)\n",
        "\n",
        "# Step 12: Perform 5-fold cross-validation with checkpointing\n",
        "skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=42)\n",
        "fold_results = []\n",
        "last_completed_fold = load_checkpoint()\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(skf.split(X_train, y_train), 1):\n",
        "    if fold <= last_completed_fold:\n",
        "        print(f\"Skipping fold {fold} (already completed)\")\n",
        "        fold_str = f\"_fold_{fold}\"\n",
        "        result_file = f'{OUTPUT_DIR}/{MODEL_NAME}_results{fold_str}.json'\n",
        "        if os.path.exists(result_file):\n",
        "            with open(result_file, 'r') as f:\n",
        "                fold_results.append(json.load(f))\n",
        "        continue\n",
        "\n",
        "    print(f\"\\nTraining Fold {fold}/{N_FOLDS}\")\n",
        "\n",
        "    X_fold_train, X_fold_val = X_train[train_idx], X_train[val_idx]\n",
        "    y_fold_train, y_fold_val = y_train[train_idx], y_train[val_idx]\n",
        "    train_ds = (\n",
        "        tf.data.Dataset.from_tensor_slices((X_fold_train, y_fold_train))\n",
        "        .batch(BATCH_SIZE)\n",
        "        .map(preprocess)\n",
        "        .map(augment_image)\n",
        "        .prefetch(tf.data.AUTOTUNE)\n",
        "    )\n",
        "\n",
        "    val_ds = (\n",
        "        tf.data.Dataset.from_tensor_slices((X_fold_val, y_fold_val))\n",
        "        .batch(BATCH_SIZE)\n",
        "        .map(preprocess)\n",
        "        .prefetch(tf.data.AUTOTUNE)\n",
        "    )\n",
        "\n",
        "    model = create_model()\n",
        "    fold_str = f\"_fold_{fold}\"\n",
        "    checkpoint_path = f'{OUTPUT_DIR}/{MODEL_NAME}_model{fold_str}.weights.h5'\n",
        "    if os.path.exists(checkpoint_path):\n",
        "        print(f\"Loading weights for fold {fold} from {checkpoint_path}\")\n",
        "        model.load_weights(checkpoint_path)\n",
        "\n",
        "    checkpoint_callback = ModelCheckpoint(checkpoint_path, monitor='val_loss', save_best_only=True, save_weights_only=True)\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "    history = model.fit(train_ds, validation_data=val_ds, epochs=EPOCHS, class_weight={0: 1.0, 1: 1.5}, verbose=1, callbacks=[early_stopping, checkpoint_callback])\n",
        "\n",
        "    results = save_model_results(\n",
        "        model, val_ds, history, MODEL_NAME, OUTPUT_DIR,\n",
        "        fold=fold, preprocessing=PREPROCESSING, hyperparameters=HYPERPARAMETERS\n",
        "    )\n",
        "    fold_results.append(results)\n",
        "\n",
        "    save_checkpoint(fold)\n",
        "\n",
        "# Step 13: Train final model on full training set\n",
        "if last_completed_fold < N_FOLDS + 1:\n",
        "    print(\"\\nTraining final model on full training set\")\n",
        "    final_model_path = f'{OUTPUT_DIR}/{MODEL_NAME}_model.keras'\n",
        "    if os.path.exists(final_model_path) and last_completed_fold == 'final':\n",
        "        print(f\"Final model already saved at {final_model_path}, skipping training\")\n",
        "        result_file = f'{OUTPUT_DIR}/{MODEL_NAME}_results.json'\n",
        "        if os.path.exists(result_file):\n",
        "            with open(result_file, 'r') as f:\n",
        "                results = json.load(f)\n",
        "            print(f\"Final Results for {MODEL_NAME}:\", results)\n",
        "    else:\n",
        "        train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(BATCH_SIZE).map(preprocess).map(augment_image).prefetch(tf.data.AUTOTUNE)\n",
        "        model = create_model()\n",
        "        checkpoint_path = f'{OUTPUT_DIR}/{MODEL_NAME}_model.weights.h5'\n",
        "        if os.path.exists(checkpoint_path):\n",
        "            print(f\"Loading weights for final model from {checkpoint_path}\")\n",
        "            model.load_weights(checkpoint_path)\n",
        "\n",
        "        checkpoint_callback = ModelCheckpoint(checkpoint_path, monitor='loss', save_best_only=True, save_weights_only=True)\n",
        "        history = model.fit(train_ds, epochs=EPOCHS, class_weight={0: 1.0, 1: 1.5}, verbose=1, callbacks=[checkpoint_callback])\n",
        "\n",
        "        results = save_model_results(\n",
        "            model, test_ds, history, MODEL_NAME, OUTPUT_DIR,\n",
        "            preprocessing=PREPROCESSING, hyperparameters=HYPERPARAMETERS\n",
        "        )\n",
        "        print(f\"Final Results for {MODEL_NAME}:\", results)\n",
        "\n",
        "        save_checkpoint('final')\n",
        "\n",
        "# Step 14: Aggregate cross-validation results\n",
        "if fold_results:\n",
        "    fold_df = pd.DataFrame(fold_results)\n",
        "    mean_results = {\n",
        "        'Model': MODEL_NAME,\n",
        "        'Preprocessing': PREPROCESSING,\n",
        "        'Mean_Accuracy': round(fold_df['Accuracy'].mean(), 2),\n",
        "        'Std_Accuracy': round(fold_df['Accuracy'].std(), 2),\n",
        "        'Mean_Precision_recaptured': round(fold_df['Precision_recaptured'].mean(), 2),\n",
        "        'Mean_Recall_recaptured': round(fold_df['Recall_recaptured'].mean(), 2),\n",
        "        'Mean_F1-Score_recaptured': round(fold_df['F1-Score_recaptured'].mean(), 2),\n",
        "        'Mean_Total_Parameters': int(fold_df['Total_Parameters'].mean()),\n",
        "        'Mean_Trainable_Parameters': int(fold_df['Trainable_Parameters'].mean())\n",
        "    }\n",
        "    with open(f'{OUTPUT_DIR}/{MODEL_NAME}_cv_summary.json', 'w') as f:\n",
        "        json.dump(mean_results, f, indent=4, default=convert_to_serializable)\n",
        "    fold_df.to_csv(f'{OUTPUT_DIR}/{MODEL_NAME}_cv_results.csv', index=False)\n",
        "    print(\"\\nCross-Validation Summary:\", mean_results)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QGcPQWA6_lxD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}