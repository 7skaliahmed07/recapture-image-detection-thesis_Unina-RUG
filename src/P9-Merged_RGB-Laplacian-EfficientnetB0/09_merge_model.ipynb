{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iWqzH4QH3g9I",
        "outputId": "2a88319a-cce4-4d94-a17a-6d32c15e7ff8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaning /content/drive...\n",
            "Mount point cleaned\n",
            "Mounted at /content/drive\n",
            "Creating virtual merge at: /content/merged_dataset\n",
            "Linking files...\n",
            "\n",
            "Linking results:\n",
            "NTU Originals: 1202 files\n",
            "Nothing_2a Originals: 106 files\n",
            "NTU Recaptures: 1199 files\n",
            "Nothing_2a Recaptures: 103 files\n",
            "\n",
            "Final merged dataset:\n",
            "Total Originals: 1308 files\n",
            "Total Recaptures: 1302 files\n",
            "\n",
            "Sample NTU originals:\n",
            "  ntu_MjuAcer_013.JPG\n",
            "  ntu_OlymNec_042.JPG\n",
            "  ntu_OlympusE500_070.JPG\n",
            "  ntu_MjuNec_071.JPG\n",
            "  ntu_OlymNec_041.JPG\n",
            "\n",
            "Sample Nothing_2a originals:\n",
            "  nothing_IMG_20251002_160015732.jpg\n",
            "  nothing_IMG_20251007_190319031.jpg\n",
            "  nothing_IMG_20250726_190213518.jpg\n",
            "  nothing_IMG_20251002_160021521.jpg\n",
            "  nothing_IMG_20251007_174427204.jpg\n",
            "\n",
            "Sample NTU recaptures:\n",
            "  ntu_zLumixD1Outdoor_071.jpg\n",
            "  ntu_zCasioIndoor_076.JPG\n",
            "  ntu_zCanonIxusIndoor_021.JPG\n",
            "  ntu_zLumixD1Indoor_063.jpg\n",
            "  ntu_zLumixD1Outdoor_063.jpg\n",
            "\n",
            "Sample Nothing_2a recaptures:\n",
            "  nothing_IMG_20251013_023758169.jpg\n",
            "  nothing_IMG_20251013_023741061.jpg\n",
            "  nothing_IMG_20251002_164942431.jpg\n",
            "  nothing_IMG_20251013_024241430.jpg\n",
            "  nothing_IMG_20251013_023810102.jpg\n",
            "\n",
            "âœ… Dataset merging completed!\n",
            "ğŸ“ Merged dataset location: /content/merged_dataset\n",
            "ğŸ“ Originals folder: /content/merged_dataset/originals\n",
            "ğŸ“ Recaptures folder: /content/merged_dataset/recaptures\n"
          ]
        }
      ],
      "source": [
        "# --------------------------------------------------------------\n",
        "#  1. MOUNT & IMPORT\n",
        "# --------------------------------------------------------------\n",
        "from google.colab import drive\n",
        "import os\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "\n",
        "# Clean the mount directory if it exists\n",
        "mount_point = '/content/drive'\n",
        "if os.path.exists(mount_point):\n",
        "    print(f\"Cleaning {mount_point}...\")\n",
        "    # Remove all files and directories in /content/drive\n",
        "    for item in Path(mount_point).iterdir():\n",
        "        if item.is_file():\n",
        "            item.unlink()\n",
        "        elif item.is_dir():\n",
        "            shutil.rmtree(item)\n",
        "    print(\"Mount point cleaned\")\n",
        "\n",
        "# Now mount fresh\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "#  2. CREATE SYMBOLIC LINKS\n",
        "# --------------------------------------------------------------\n",
        "MERGED_ROOT = \"/content/merged_dataset\"\n",
        "\n",
        "# Clear and recreate merged dataset\n",
        "if os.path.exists(MERGED_ROOT):\n",
        "    shutil.rmtree(MERGED_ROOT)\n",
        "os.makedirs(MERGED_ROOT, exist_ok=True)\n",
        "\n",
        "def create_symlink_folder(src_folder, target_folder, prefix=\"\"):\n",
        "    if not Path(src_folder).exists():\n",
        "        print(f\"âš ï¸  Skipping {src_folder} - folder not found\")\n",
        "        return 0\n",
        "\n",
        "    os.makedirs(target_folder, exist_ok=True)\n",
        "    files_linked = 0\n",
        "\n",
        "    for item in Path(src_folder).iterdir():\n",
        "        if item.is_file():\n",
        "            # Direct file in folder (Nothing_2a structure)\n",
        "            if prefix:\n",
        "                new_name = f\"{prefix}_{item.name}\"\n",
        "            else:\n",
        "                new_name = item.name\n",
        "            link_path = Path(target_folder) / new_name\n",
        "            if not link_path.exists():\n",
        "                os.symlink(item, link_path)\n",
        "                files_linked += 1\n",
        "        elif item.is_dir():\n",
        "            # Subfolder with files (NTU structure)\n",
        "            for file_item in item.iterdir():\n",
        "                if file_item.is_file():\n",
        "                    # Create unique name using folder name\n",
        "                    if prefix:\n",
        "                        unique_name = f\"{prefix}_{item.name}_{file_item.name}\"\n",
        "                    else:\n",
        "                        unique_name = f\"{item.name}_{file_item.name}\"\n",
        "                    link_path = Path(target_folder) / unique_name\n",
        "                    if not link_path.exists():\n",
        "                        os.symlink(file_item, link_path)\n",
        "                        files_linked += 1\n",
        "\n",
        "    return files_linked\n",
        "\n",
        "print(f\"Creating virtual merge at: {MERGED_ROOT}\")\n",
        "\n",
        "# Define source folders\n",
        "ntu_orig = \"/content/drive/MyDrive/NTU-Roselab-Dataset/originals\"\n",
        "ntu_recp = \"/content/drive/MyDrive/NTU-Roselab-Dataset/recaptures\"\n",
        "mob_orig = \"/content/drive/MyDrive/Nothing_2a/originals\"\n",
        "mob_recp = \"/content/drive/MyDrive/Nothing_2a/recaptures\"\n",
        "\n",
        "# Create merged structure with prefixes to avoid conflicts\n",
        "print(\"Linking files...\")\n",
        "ntu_orig_count = create_symlink_folder(ntu_orig, f\"{MERGED_ROOT}/originals\", \"ntu\")\n",
        "mob_orig_count = create_symlink_folder(mob_orig, f\"{MERGED_ROOT}/originals\", \"nothing\")\n",
        "ntu_recp_count = create_symlink_folder(ntu_recp, f\"{MERGED_ROOT}/recaptures\", \"ntu\")\n",
        "mob_recp_count = create_symlink_folder(mob_recp, f\"{MERGED_ROOT}/recaptures\", \"nothing\")\n",
        "\n",
        "print(f\"\\nLinking results:\")\n",
        "print(f\"NTU Originals: {ntu_orig_count} files\")\n",
        "print(f\"Nothing_2a Originals: {mob_orig_count} files\")\n",
        "print(f\"NTU Recaptures: {ntu_recp_count} files\")\n",
        "print(f\"Nothing_2a Recaptures: {mob_recp_count} files\")\n",
        "\n",
        "print(f\"\\nFinal merged dataset:\")\n",
        "print(f\"Total Originals: {len(list(Path(f'{MERGED_ROOT}/originals').glob('*')))} files\")\n",
        "print(f\"Total Recaptures: {len(list(Path(f'{MERGED_ROOT}/recaptures').glob('*')))} files\")\n",
        "\n",
        "# Show sample files from each source\n",
        "print(f\"\\nSample NTU originals:\")\n",
        "ntu_originals = [f for f in Path(f'{MERGED_ROOT}/originals').glob('ntu_*')]\n",
        "for f in ntu_originals[:5]:\n",
        "    print(f\"  {f.name}\")\n",
        "\n",
        "print(f\"\\nSample Nothing_2a originals:\")\n",
        "nothing_originals = [f for f in Path(f'{MERGED_ROOT}/originals').glob('nothing_*')]\n",
        "for f in nothing_originals[:5]:\n",
        "    print(f\"  {f.name}\")\n",
        "\n",
        "print(f\"\\nSample NTU recaptures:\")\n",
        "ntu_recaptures = [f for f in Path(f'{MERGED_ROOT}/recaptures').glob('ntu_*')]\n",
        "for f in ntu_recaptures[:5]:\n",
        "    print(f\"  {f.name}\")\n",
        "\n",
        "print(f\"\\nSample Nothing_2a recaptures:\")\n",
        "nothing_recaptures = [f for f in Path(f'{MERGED_ROOT}/recaptures').glob('nothing_*')]\n",
        "for f in nothing_recaptures[:5]:\n",
        "    print(f\"  {f.name}\")\n",
        "\n",
        "print(f\"\\nâœ… Dataset merging completed!\")\n",
        "print(f\"ğŸ“ Merged dataset location: {MERGED_ROOT}\")\n",
        "print(f\"ğŸ“ Originals folder: {MERGED_ROOT}/originals\")\n",
        "print(f\"ğŸ“ Recaptures folder: {MERGED_ROOT}/recaptures\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Downloading Model\n"
      ],
      "metadata": {
        "id": "vW5VcqQD-U3b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------------------------------------------\n",
        "#  DOWNLOAD EfficientNetB0 weights ONCE & SAVE TO DRIVE\n",
        "# --------------------------------------------------------------\n",
        "import urllib.request\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# URL and local path\n",
        "URL = \"https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\"\n",
        "SAVE_PATH = \"/content/drive/MyDrive/efficientnetb0_notop.h5\"\n",
        "\n",
        "# Download with progress bar\n",
        "print(\"Downloading EfficientNetB0 weights (~16MB)...\")\n",
        "def download_with_progress(url, filepath):\n",
        "    if os.path.exists(filepath):\n",
        "        print(f\"Already exists: {filepath}\")\n",
        "        return filepath\n",
        "    with urllib.request.urlopen(url) as response, open(filepath, 'wb') as out_file:\n",
        "        length = int(response.getheader('content-length', 0))\n",
        "        downloaded = 0\n",
        "        chunk_size = 1024 * 1024\n",
        "        while True:\n",
        "            data = response.read(chunk_size)\n",
        "            if not data:\n",
        "                break\n",
        "            out_file.write(data)\n",
        "            downloaded += len(data)\n",
        "            if length > 0:\n",
        "                print(f\"\\rProgress: {downloaded / length:.1%}\", end=\"\")\n",
        "    print(f\"\\nSaved to: {filepath}\")\n",
        "    return filepath\n",
        "\n",
        "try:\n",
        "    download_with_progress(URL, SAVE_PATH)\n",
        "except Exception as e:\n",
        "    print(\"Failed. Trying alternative mirror...\")\n",
        "    # Alternative mirror (GitHub)\n",
        "    URL_ALT = \"https://github.com/keras-team/keras-applications/releases/download/efficientnet/efficientnetb0_notop.h5\"\n",
        "    download_with_progress(URL_ALT, SAVE_PATH)\n",
        "\n",
        "print(\"Weights ready for local loading!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LVPu7jyG-T0g",
        "outputId": "65cd690c-08e7-4f1f-8d8e-5ad2ac665312"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading EfficientNetB0 weights (~16MB)...\n",
            "Progress: 100.0%\n",
            "Saved to: /content/drive/MyDrive/efficientnetb0_notop.h5\n",
            "Weights ready for local loading!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Replace Model Creation with Local Weight Loading"
      ],
      "metadata": {
        "id": "onGwKhcw-kXy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------------------------------------------\n",
        "#  MODEL â€“ LOAD WEIGHTS FROM DRIVE (NO INTERNET NEEDED AFTER)\n",
        "# --------------------------------------------------------------\n",
        "WEIGHTS_PATH = \"/content/drive/MyDrive/efficientnetb0_notop.h5\"\n",
        "\n",
        "def create_model():\n",
        "    rgb_in = Input((IMG_SIZE, IMG_SIZE, 3), name='rgb')\n",
        "    lap_in = Input((IMG_SIZE, IMG_SIZE, 3), name='lap')\n",
        "\n",
        "    # Use weights=None â†’ we'll load manually\n",
        "    b1 = applications.EfficientNetB0(include_top=False, weights=None, name='eff_rgb')(rgb_in)\n",
        "    b1 = GlobalAveragePooling2D()(b1)\n",
        "    b2 = applications.EfficientNetB0(include_top=False, weights=None, name='eff_lap')(lap_in)\n",
        "    b2 = GlobalAveragePooling2D()(b2)\n",
        "\n",
        "    x = Concatenate()([b1, b2])\n",
        "    x = Dense(128, activation='relu')(x)\n",
        "    x = Dropout(0.4)(x)\n",
        "    out = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    m = Model([rgb_in, lap_in], out)\n",
        "    m.compile(optimizer=tf.keras.optimizers.Adam(1e-4),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "    # Load weights from local file\n",
        "    print(\"Loading EfficientNet weights from Drive...\")\n",
        "    m.get_layer('eff_rgb').load_weights(WEIGHTS_PATH, by_name=True, skip_mismatch=True)\n",
        "    m.get_layer('eff_lap').load_weights(WEIGHTS_PATH, by_name=True, skip_mismatch=True)\n",
        "    print(\"Weights loaded!\")\n",
        "\n",
        "    return m\n",
        "\n",
        "model = create_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xSmkx2aI-kiS",
        "outputId": "12a00d6e-2aee-4778-e2c4-40b1c7ba0b9c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading EfficientNet weights from Drive...\n",
            "Weights loaded!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Merged Model"
      ],
      "metadata": {
        "id": "zITIaWgc8VcQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------------------------------------------\n",
        "# 0. MOUNT DRIVE\n",
        "# --------------------------------------------------------------\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# 1. IMPORTS\n",
        "# --------------------------------------------------------------\n",
        "import os, json, urllib.request, numpy as np, tensorflow as tf\n",
        "from pathlib import Path\n",
        "from tensorflow.keras import applications, callbacks, layers, models\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout, Concatenate, GlobalAveragePooling2D\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.utils import image_dataset_from_directory\n",
        "from PIL import Image\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# 2. GLOBAL SETTINGS\n",
        "# --------------------------------------------------------------\n",
        "MERGED_ROOT      = \"/content/merged_dataset\"\n",
        "WEIGHTS_PATH     = \"/content/drive/MyDrive/efficientnetb0_notop.h5\"\n",
        "IMG_SIZE         = 224\n",
        "BATCH_SIZE       = 16\n",
        "EPOCHS           = 15\n",
        "OUTPUT_DIR       = \"/content/drive/MyDrive/Merged_Model/results\"\n",
        "Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "CHECKPOINT_DIR   = OUTPUT_DIR                     # full-model files live here\n",
        "CHECKPOINT_PTR   = f\"{OUTPUT_DIR}/latest_checkpoint.json\"\n",
        "HISTORY_FILE     = f\"{OUTPUT_DIR}/history.json\"\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# 3. (ONE-TIME) DOWNLOAD EFFICIENTNET WEIGHTS\n",
        "# --------------------------------------------------------------\n",
        "def download_weights():\n",
        "    if os.path.exists(WEIGHTS_PATH):\n",
        "        print(f\"Weights ready: {WEIGHTS_PATH}\")\n",
        "        return\n",
        "    print(\"Downloading EfficientNetB0 weights...\")\n",
        "    url = \"https://github.com/keras-team/keras-applications/releases/download/efficientnet/efficientnetb0_notop.h5\"\n",
        "    urllib.request.urlretrieve(url, WEIGHTS_PATH)\n",
        "    print(\"Saved to Drive\")\n",
        "\n",
        "download_weights()\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# 4. AUTO-REBUILD merged_dataset (runs every session)\n",
        "# --------------------------------------------------------------\n",
        "def build_merged_dataset():\n",
        "    if os.path.isdir(MERGED_ROOT) and any(Path(MERGED_ROOT).iterdir()):\n",
        "        print(f\"Using existing merged dataset: {MERGED_ROOT}\")\n",
        "        return\n",
        "    print(f\"Building merged dataset at: {MERGED_ROOT}\")\n",
        "\n",
        "    ntu_orig = \"/content/drive/MyDrive/NTU-Roselab-Dataset/originals\"\n",
        "    ntu_recp = \"/content/drive/MyDrive/NTU-Roselab-Dataset/recaptures\"\n",
        "    mob_orig = \"/content/drive/MyDrive/Nothing_2a/originals\"\n",
        "    mob_recp = \"/content/drive/MyDrive/Nothing_2a/recaptures\"\n",
        "\n",
        "    def link_folder(src, dst, prefix):\n",
        "        if not Path(src).exists():\n",
        "            print(f\"Warning: Skipping {src} (not found)\")\n",
        "            return 0\n",
        "        os.makedirs(dst, exist_ok=True)\n",
        "        cnt = 0\n",
        "        for item in Path(src).iterdir():\n",
        "            if item.is_file():\n",
        "                name = f\"{prefix}_{item.name}\"\n",
        "                link = Path(dst) / name\n",
        "                if not link.exists():\n",
        "                    os.symlink(item, link)\n",
        "                    cnt += 1\n",
        "        return cnt\n",
        "\n",
        "    os.makedirs(MERGED_ROOT, exist_ok=True)\n",
        "    orig = link_folder(ntu_orig, f\"{MERGED_ROOT}/originals\", \"ntu\")\n",
        "    orig += link_folder(mob_orig, f\"{MERGED_ROOT}/originals\", \"nothing\")\n",
        "    recp = link_folder(ntu_recp, f\"{MERGED_ROOT}/recaptures\", \"ntu\")\n",
        "    recp += link_folder(mob_recp, f\"{MERGED_ROOT}/recaptures\", \"nothing\")\n",
        "    print(f\"Merged: {orig} originals, {recp} recaptures\")\n",
        "\n",
        "build_merged_dataset()\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# 5. PRE-PROCESSING\n",
        "# --------------------------------------------------------------\n",
        "@tf.function\n",
        "def rgb_preprocess(img):\n",
        "    img = tf.image.resize(img, (IMG_SIZE, IMG_SIZE))\n",
        "    return tf.keras.applications.efficientnet.preprocess_input(img)\n",
        "\n",
        "@tf.function\n",
        "def laplacian_preprocess(img):\n",
        "    img = tf.image.resize(img, (IMG_SIZE, IMG_SIZE))\n",
        "    img = tf.image.rgb_to_grayscale(img)\n",
        "    kernel = tf.constant([[0,-1,0],[-1,4,-1],[0,-1,0]], dtype=tf.float32)\n",
        "    kernel = tf.reshape(kernel, [3,3,1,1])\n",
        "    filtered = tf.nn.conv2d(img, kernel, strides=1, padding='SAME')\n",
        "    filtered = tf.abs(filtered)\n",
        "    mn, mx = tf.reduce_min(filtered), tf.reduce_max(filtered)\n",
        "    filtered = (filtered - mn) / (mx - mn + 1e-6)\n",
        "    return tf.tile(filtered, [1,1,1,3])\n",
        "\n",
        "@tf.function\n",
        "def preprocess_dual(img, label):\n",
        "    return (rgb_preprocess(img), laplacian_preprocess(img)), label\n",
        "\n",
        "@tf.function\n",
        "def augment(inputs, label):\n",
        "    rgb, lap = inputs\n",
        "    rgb = tf.image.random_flip_left_right(rgb)\n",
        "    lap = tf.image.random_flip_left_right(lap)\n",
        "    k = tf.random.uniform([], 0, 4, tf.int32)\n",
        "    rgb = tf.image.rot90(rgb, k)\n",
        "    lap = tf.image.rot90(lap, k)\n",
        "    return (rgb, lap), label\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# 6. BUILD DATASETS\n",
        "# --------------------------------------------------------------\n",
        "train_ds = image_dataset_from_directory(\n",
        "    MERGED_ROOT,\n",
        "    label_mode='binary',\n",
        "    image_size=(IMG_SIZE, IMG_SIZE),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    validation_split=0.2,\n",
        "    subset='training',\n",
        "    seed=42,\n",
        "    shuffle=True\n",
        ").map(preprocess_dual).map(augment).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "val_ds = image_dataset_from_directory(\n",
        "    MERGED_ROOT,\n",
        "    label_mode='binary',\n",
        "    image_size=(IMG_SIZE, IMG_SIZE),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    validation_split=0.2,\n",
        "    subset='validation',\n",
        "    seed=42\n",
        ").map(preprocess_dual).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "print(f\"Train batches: {len(train_ds)} | Val batches: {len(val_ds)}\")\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# 7. MODEL + RESUME LOGIC\n",
        "# --------------------------------------------------------------\n",
        "def create_model():\n",
        "    rgb_in = Input((IMG_SIZE, IMG_SIZE, 3), name='rgb')\n",
        "    lap_in = Input((IMG_SIZE, IMG_SIZE, 3), name='lap')\n",
        "\n",
        "    b1 = applications.EfficientNetB0(include_top=False, weights=None, name='eff_rgb')(rgb_in)\n",
        "    b1 = GlobalAveragePooling2D()(b1)\n",
        "    b2 = applications.EfficientNetB0(include_top=False, weights=None, name='eff_lap')(lap_in)\n",
        "    b2 = GlobalAveragePooling2D()(b2)\n",
        "\n",
        "    x = Concatenate()([b1, b2])\n",
        "    x = Dense(128, activation='relu')(x)\n",
        "    x = Dropout(0.4)(x)\n",
        "    out = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    m = Model([rgb_in, lap_in], out)\n",
        "    m.compile(optimizer=tf.keras.optimizers.Adam(1e-4),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy', 'Precision', 'Recall'])\n",
        "\n",
        "    print(\"Loading EfficientNet backbone from Drive...\")\n",
        "    m.get_layer('eff_rgb').load_weights(WEIGHTS_PATH, by_name=True, skip_mismatch=True)\n",
        "    m.get_layer('eff_lap').load_weights(WEIGHTS_PATH, by_name=True, skip_mismatch=True)\n",
        "    print(\"Backbone loaded\")\n",
        "    return m\n",
        "\n",
        "def load_latest_checkpoint():\n",
        "    \"\"\"Return (model, start_epoch, history_dict)\"\"\"\n",
        "    if not os.path.exists(CHECKPOINT_PTR):\n",
        "        print(\"No checkpoint â†’ starting fresh\")\n",
        "        return create_model(), 0, {}\n",
        "\n",
        "    with open(CHECKPOINT_PTR) as f:\n",
        "        info = json.load(f)\n",
        "    cp_path = info['path']\n",
        "    epoch   = info['epoch']          # 0-based\n",
        "    print(f\"Resuming from {cp_path} (epoch {epoch+1})\")\n",
        "    model = tf.keras.models.load_model(cp_path)\n",
        "\n",
        "    hist = {}\n",
        "    if os.path.exists(HISTORY_FILE):\n",
        "        with open(HISTORY_FILE) as f:\n",
        "            hist = json.load(f)\n",
        "    return model, epoch, hist\n",
        "\n",
        "model, start_epoch, history_dict = load_latest_checkpoint()\n",
        "print(f\"Training from epoch {start_epoch+1}/{EPOCHS}\")\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# 8. CALLBACK â€“ SAVE FULL MODEL + POINTER + HISTORY\n",
        "# --------------------------------------------------------------\n",
        "class FullCheckpoint(callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        epoch += 1                      # make 1-based for filename\n",
        "        cp_path = f\"{CHECKPOINT_DIR}/epoch_{epoch:02d}.keras\"\n",
        "        self.model.save(cp_path, overwrite=True)\n",
        "\n",
        "        # update pointer file\n",
        "        with open(CHECKPOINT_PTR, 'w') as f:\n",
        "            json.dump({'path': cp_path, 'epoch': epoch-1}, f)   # epoch-1 = 0-based\n",
        "\n",
        "        # append logs to history file\n",
        "        logs = logs or {}\n",
        "        for k, v in logs.items():\n",
        "            history_dict.setdefault(k, []).append(float(v))\n",
        "        with open(HISTORY_FILE, 'w') as f:\n",
        "            json.dump(history_dict, f, indent=2)\n",
        "\n",
        "full_cp = FullCheckpoint()\n",
        "es = EarlyStopping(monitor='val_accuracy', patience=4,\n",
        "                   restore_best_weights=True, mode='max', verbose=1)\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# 9. TRAIN (will continue from start_epoch)\n",
        "# --------------------------------------------------------------\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=EPOCHS,\n",
        "    initial_epoch=start_epoch,\n",
        "    callbacks=[full_cp, es],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# 10. FINAL SAVE (best model)\n",
        "# --------------------------------------------------------------\n",
        "final_path = f\"{OUTPUT_DIR}/final_merged_model.keras\"\n",
        "model.save(final_path)\n",
        "print(f\"\\nTRAINING FINISHED â€“ final model saved to:\\n{final_path}\")\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# 11. QUICK TEST\n",
        "# --------------------------------------------------------------\n",
        "def predict_image(path):\n",
        "    img = Image.open(path).convert('RGB').resize((IMG_SIZE, IMG_SIZE))\n",
        "    x = np.array(img, dtype=np.float32)[None, ...]\n",
        "    rgb = rgb_preprocess(x)\n",
        "    lap = laplacian_preprocess(x)\n",
        "    p = model.predict([rgb, lap], verbose=0)[0,0]\n",
        "    label = \"Recaptured\" if p > 0.5 else \"Original\"\n",
        "    print(f\"{os.path.basename(path)} â†’ {label} ({p:.1%})\")\n",
        "\n",
        "# Test a recapture from your Android folder\n",
        "predict_image(\"/content/drive/MyDrive/Nothing_2a/recaptures/IMG_20251002_164912794.jpg\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hpnU9VhB-wdW",
        "outputId": "15dfea7c-b4a1-4f4d-f022-d746affb9c57"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Weights ready: /content/drive/MyDrive/efficientnetb0_notop.h5\n",
            "Using existing merged dataset: /content/merged_dataset\n",
            "Found 209 files belonging to 2 classes.\n",
            "Using 168 files for training.\n",
            "Found 209 files belonging to 2 classes.\n",
            "Using 41 files for validation.\n",
            "Train batches: 11 | Val batches: 3\n",
            "No checkpoint â†’ starting fresh\n",
            "Loading EfficientNet backbone from Drive...\n",
            "Backbone loaded\n",
            "Training from epoch 1/15\n",
            "Epoch 1/15\n",
            "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m282s\u001b[0m 11s/step - Precision: 0.4599 - Recall: 0.6279 - accuracy: 0.4849 - loss: 0.7316 - val_Precision: 0.5278 - val_Recall: 0.9048 - val_accuracy: 0.5366 - val_loss: 0.7058\n",
            "Epoch 2/15\n",
            "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 5s/step - Precision: 0.6660 - Recall: 0.8260 - accuracy: 0.7129 - loss: 0.6074 - val_Precision: 0.5455 - val_Recall: 0.8571 - val_accuracy: 0.5610 - val_loss: 0.6921\n",
            "Epoch 3/15\n",
            "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 5s/step - Precision: 0.8211 - Recall: 0.8578 - accuracy: 0.8544 - loss: 0.4745 - val_Precision: 0.6071 - val_Recall: 0.8095 - val_accuracy: 0.6341 - val_loss: 0.6759\n",
            "Epoch 4/15\n",
            "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 5s/step - Precision: 0.8855 - Recall: 0.8296 - accuracy: 0.8613 - loss: 0.4352 - val_Precision: 0.5667 - val_Recall: 0.8095 - val_accuracy: 0.5854 - val_loss: 0.6705\n",
            "Epoch 5/15\n",
            "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 5s/step - Precision: 0.8902 - Recall: 0.8156 - accuracy: 0.8573 - loss: 0.3734 - val_Precision: 0.6538 - val_Recall: 0.8095 - val_accuracy: 0.6829 - val_loss: 0.6676\n",
            "Epoch 6/15\n",
            "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 6s/step - Precision: 0.9245 - Recall: 0.7722 - accuracy: 0.8466 - loss: 0.3485 - val_Precision: 0.6400 - val_Recall: 0.7619 - val_accuracy: 0.6585 - val_loss: 0.6587\n",
            "Epoch 7/15\n",
            "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 5s/step - Precision: 0.9547 - Recall: 0.9419 - accuracy: 0.9512 - loss: 0.2277 - val_Precision: 0.6190 - val_Recall: 0.6190 - val_accuracy: 0.6098 - val_loss: 0.6581\n",
            "Epoch 8/15\n",
            "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 5s/step - Precision: 0.9361 - Recall: 0.9455 - accuracy: 0.9480 - loss: 0.2186 - val_Precision: 0.6000 - val_Recall: 0.5714 - val_accuracy: 0.5854 - val_loss: 0.6762\n",
            "Epoch 9/15\n",
            "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 5s/step - Precision: 0.9697 - Recall: 0.9799 - accuracy: 0.9751 - loss: 0.1601 - val_Precision: 0.6087 - val_Recall: 0.6667 - val_accuracy: 0.6098 - val_loss: 0.6915\n",
            "Epoch 9: early stopping\n",
            "Restoring model weights from the end of the best epoch: 5.\n",
            "\n",
            "TRAINING FINISHED â€“ final model saved to:\n",
            "/content/drive/MyDrive/Merged_Model/results/final_merged_model.keras\n",
            "IMG_20251002_164912794.jpg â†’ Original (28.2%)\n"
          ]
        }
      ]
    }
  ]
}